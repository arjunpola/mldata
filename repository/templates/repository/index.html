{% extends "repository/base.html" %}
{% load i18n %}

{% block content %}
<h1>{% trans "Repository" %}</h1>
<p>{% trans "This is the repository where you can find and modify all available data, tasks and solutions." %}</p>

<h2>{% trans "Latest Items" %}</h2>
<ul>
	{% if latest_data %}<li>{% trans "Data" %}: <a href="{% url repository.views.data_view_main latest_data.slug.text %}">{{ latest_data.name }}</a></li>{%endif %}
	{% if latest_task %}<li>{% trans "Task" %}: <a href="{% url repository.views.task_view latest_task.id %}">{{ latest_task.name }}</a></li>{%endif %}
	{% if latest_solution %}<li>{% trans "Solution" %}: <a href="{% url repository.views.solution_view latest_task.id %}">{{ latest_solution.name }}</a></li>{%endif %}
</ul>

<p>The following kinds of objects are stored in the repository. We will illustrate each class with the MNIST5 data set:</p>
<ul>
	<li><strong>Data</strong> This is &quot;raw data&quot;, available in any one of a number of standard formats (including, for example, CSV, ARFF, netCDF, HDF5, ODBC), together with a description of the data, sources, possibly publications. For MNIST, the data is the images with corresponding labels.</li>
	<li><strong>Task</strong> This is a learning task, consisting of a formal description of input-output relationships based on a data set and methods for evaluating the results. This might also include fixed partitionings into training and test data for the base data set. For the MNIST example, a learning task might be to perform multiclass classification for all ten classes, using the F1-score as performance measure.</li>
	<li><strong>Solution</strong> Essentially, a solution consists of a specific machine learning method applied to a learning task, but including all the other information, for example, what kind of feature processing has been applied, how parameters of the learning method have been selected, what operating system has been used, etc. Ideally, a solution also provides a single parameterless piece of code for others to re-run the experiments, preferably with source code.</li>
</ul>

<p>These descriptions boil down to the following properties for the respective object classes:</p>
<ul>
	<li>Common to all: papers, external urls, free form description, version, time stamps, possibly also back links</li>
	<li>Data: container format, source url, measurement details, usage scenario, links to tasks, links to solutions</li>
	<li>Task: input format, output format, performance measure, links to data, links to solutions</li>
	<li>Solution: numerical scores for given data and task, computational pipeline</li>
</ul>

<p>Between tasks and data sets, the relationship is many-to-many, as a data set can give rise to many different learning tasks, as well as a learning task potentially referencing several data sets. On the other hand, a task can have many solutions, but each solution belongs to a certain learning task.</p>
<p>Given the described infrastructure, organising a challenge essentially boils down to selecting a group of datasets and appropriate data splits into training validation and test data, and choosing a machine learning task. As the data storage and evaluating mechanism is already taken care of by the basic data-repository one merely needs to add an administration interface and a challenge &quot;view&quot; on the data.</p>

{% endblock %}

